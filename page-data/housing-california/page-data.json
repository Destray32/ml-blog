{"componentChunkName":"component---src-templates-blog-post-js","path":"/housing-california/","result":{"data":{"markdownRemark":{"html":"<h2>Introduction</h2>\n<p>Lately I had a great opportunity to read the book <em>\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"</em> by Aurélien Géron<sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup>. The book is a great resource for learning machine learning and deep learning.</p>\n<p>One of the projects in the book is to predict the housing prices using the California housing dataset. I decided to implement the project in Python and see what I can learn from it.</p>\n<p>Code examples for this project can be found in the GitHub Repository<sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup> of the author.</p>\n<h2>Dataset description</h2>\n<p>The dataset of housing prices using the California housing dataset from StatLib repository<sup id=\"fnref-3\"><a href=\"#fn-3\" class=\"footnote-ref\">3</a></sup> with data that was based on data from the 1990 California census. The dataset has 20,640 observations on housing prices, which is not a big dataset but it is a good enough to learn from.</p>\n<p>The dataset has the following features:</p>\n<ol>\n<li><code>longitude</code>: A measure of how far west a house is; a higher value is farther west</li>\n<li><code>latitude</code>: A measure of how far north a house is; a higher value is farther north</li>\n<li><code>housing_median_age</code>: Median age of a house within a block; a lower number is a newer building</li>\n<li><code>total_rooms</code>: Total number of rooms within a block</li>\n<li><code>total_bedrooms</code>: Total number of bedrooms within a block</li>\n<li><code>population</code>: Total number of people residing within a block</li>\n<li><code>households</code>: Total number of households, a group of people residing within a home unit, for a block</li>\n<li><code>median_income</code>: Median income for households within a block of houses</li>\n<li><code>median_house_value</code>: Median house value for households within a block</li>\n</ol>\n<h2>Exploratory Data Analysis</h2>\n<p>We will start by loading the dataset and exploring date to get a better understanding of it. My favourite step is to plot the data using real california map to see the distribution of houses. This will help us to understand the data better.</p>\n<p>We can see that this chart shows the median house value in blue to red color scale. If it is blue then the house median value is low and red if it is otherwise.</p>\n<p><img src=\"https://imgur.com/QCZ90M7.png\" alt=\"houses-visualisation\"></p>\n<h3>Looking for correlations</h3>\n<p>Out data is not very large so we can easily calculate the standard correlation coefficient (Pearson's r) between every pair of attributes using the <code>corr()</code> method.</p>\n<pre><code class=\"language-python\">corr_matrix = housing.corr()\r\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n</code></pre>\n<p>This will give us the following correlation matrix:</p>\n<pre><code>median_house_value    1.000000\r\nmedian_income         0.688380\r\ntotal_rooms           0.137455\r\nhousing_median_age    0.102175\r\nhouseholds            0.071426\r\ntotal_bedrooms        0.054635\r\npopulation           -0.020153\r\nlongitude            -0.050859\r\nlatitude             -0.139584\r\nName: median_house_value, dtype: float64\n</code></pre>\n<p>As we can see the <code>median_income</code> has the highest correlation with the <code>median_house_value</code> which is 0.688380 and the latitude has the lowest correlation with the <code>median_house_value</code> which is -0.139584. This means that <code>median_income</code> will propably be the most important feature for predicting the <code>median_house_value</code>.</p>\n<p>We can try to plot the scatter matrix to see the correlation between the features.</p>\n<p><img src=\"https://i.imgur.com/IUxKChE.png\" alt=\"scatter-matrix\"></p>\n<p>From there we can clearly see that the <code>median_income</code> has the most linear correlation with the <code>median_house_value</code>. So we plot the scatter plot median_income against median_house_value.</p>\n<p><img src=\"https://i.imgur.com/bJNGYH7.png\" alt=\"median income agains value\"></p>\n<p>The interesting thing with this plot is that we can clearly see some horizontal lines at 500,000, 450,000, 350,000 and 280,000. This means that the data is capped at these values. We can try to remove these data points from the dataset to avoid the model to learn from these data points.</p>\n<h2>Feature Engineering</h2>\n<h3>Handling text/categorical attributes</h3>\n<p>Our dataset contains one categorical attribute <code>ocean_proximity</code>. We can convert these text labels to numbers using the <code>OneHotEncoder</code> class from Scikit-Learn.</p>\n<pre><code class=\"language-python\">from sklearn.preprocessing import OrdinalEncoder\r\n\r\nhousing_cat = housing[[\"ocean_proximity\"]]\r\n\r\nordinal_encoder = OrdinalEncoder()\r\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n</code></pre>\n<p>This will convert the text labels to numbers.</p>\n<h3>Feature Scaling and custom transformers</h3>\n<p>This step is very important in this small project because the features have different scales. Most of the times machine learning algorithms tend to perform better when the features are scaled. Without scaling models will be biased towards the features with the larger scales.</p>\n<p>For this task we will create our own transformer to scale the features.</p>\n<pre><code class=\"language-python\">from sklearn.base import BaseEstimator, TransformerMixin\r\nfrom sklearn.utils.validation import check_array, check_is_fitted\r\n\r\nclass StandardScalerClone(BaseEstimator, TransformerMixin):\r\n    def __init__(self, with_mean=True):  # no *args or **kwargs!\r\n        self.with_mean = with_mean\r\n\r\n    def fit(self, X, y=None):  # y is required even though we don't use it\r\n        X = check_array(X)  # checks that X is an array with finite float values\r\n        self.mean_ = X.mean(axis=0)\r\n        self.scale_ = X.std(axis=0)\r\n        self.n_features_in_ = X.shape[1]  # allows us to check that X has the same number of features during transform\r\n        return self  # always return self!\r\n\r\n    def transform(self, X):\r\n        check_is_fitted(self)  # looks for learned attributes (with trailing _)\r\n        X = check_array(X)\r\n        assert self.n_features_in_ == X.shape[1]\r\n        if self.with_mean:\r\n            X = X - self.mean_\r\n        return X / self.scale_\n</code></pre>\n<p>And then we will create a transformer for finding clusters in the data.</p>\n<pre><code class=\"language-python\">from sklearn.cluster import KMeans\r\n\r\nclass ClusterSimilarity(BaseEstimator, TransformerMixin):\r\n    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\r\n        self.n_clusters = n_clusters\r\n        self.gamma = gamma\r\n        self.random_state = random_state\r\n\r\n    def fit(self, X, y=None, sample_weight=None): \r\n        self.kmeans_ = KMeans(self.n_clusters, n_init=10,\r\n                              random_state=self.random_state)\r\n        self.kmeans_.fit(X, sample_weight=sample_weight)\r\n        return self  # always return self!\r\n\r\n    def transform(self, X):\r\n        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\r\n\r\n    def get_feature_names_out(self, names=None):\r\n        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n</code></pre>\n<p>Later we will use these transformers in the pipeline for the model.</p>\n<p>We can check how our cluster transformer works by plotting the clusters on the map. But first we will need to fit transformer to the data.</p>\n<pre><code class=\"language-python\">cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\r\nsimilarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\r\n                                           sample_weight=housing_labels)\n</code></pre>\n<p>After plotting the clusters on the map we can see that it did a good job at finding most concentrated areas.</p>\n<p><img src=\"https://i.imgur.com/ifx2pvw.png\" alt=\"clusters\"></p>\n<p>With custom transformers we can now build the pipeline for doing all the work for our data preparation.</p>\n<pre><code class=\"language-python\">def column_ratio(X):\r\nreturn X[:, [0]] / X[:, [1]]\r\n\r\ndef ratio_name(function_transformer, feature_names_in):\r\n    return [\"ratio\"]  # feature names out\r\n\r\ndef ratio_pipeline():\r\n    return make_pipeline(\r\n        SimpleImputer(strategy=\"median\"),\r\n        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\r\n        StandardScaler())\r\n\r\nlog_pipeline = make_pipeline(\r\n    SimpleImputer(strategy=\"median\"),\r\n    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\r\n    StandardScaler())\r\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\r\ndefault_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\r\n                                     StandardScaler())\r\npreprocessing = ColumnTransformer([\r\n        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\r\n        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\r\n        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\r\n        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\r\n                               \"households\", \"median_income\"]),\r\n        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\r\n        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\r\n    ],\r\n    remainder=default_num_pipeline)  # one column remaining: housing_median_age\n</code></pre>\n<p>The following code defines three functions, first function <code>column_ratio</code> calculates the ratio of two columns. The second function <code>ratio_name</code> returns the name of the new feature. The third function <code>ratio_pipeline</code> returns the pipeline for the ratio feature.</p>\n<p>Then we define the <code>log_pipeline</code> which calculates the logarithm of the features. Next thing is to define the <code>cluster_simil</code> which is the pipeline for the cluster transformer and the last one <code>default_num_pipeline</code> which is the default pipeline for the numerical features.</p>\n<p>The last step is to define the <code>preprocessing</code> pipeline which will use all the defined pipelines for the data preparation.</p>\n<h2>Model selection and training</h2>\n<p>After exploring the data and preparing features we can now start with selecting good model for the task. We have tried the following models:</p>\n<ol>\n<li>Linear Regression</li>\n<li>Decision Tree Regressor</li>\n</ol>\n<h3>Linear Regression</h3>\n<p>With our data prepeared with preprocessing pipeline we can now train the linear regression model.</p>\n<pre><code class=\"language-python\">from sklearn.linear_model import LinearRegression\r\n\r\nlin_reg = make_pipeline(preprocessing, LinearRegression())\r\nlin_reg.fit(housing, housing_labels)\n</code></pre>\n<p>After training the model and comparing the predictions with the actual values we can see that the model is not very good with predicting correct values.</p>\n<pre><code class=\"language-python\">housing_predictions = lin_reg.predict(housing)\r\nhousing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred\r\n>>> array([242800., 375900., 127500.,  99400., 324600.])\r\n\r\nhousing_labels.iloc[:5].values\r\n>>> array([458300., 483800., 101700.,  96100., 361800.])\n</code></pre>\n<p>We can see that after calculating the mean squared error the error is 68647 which is not very good for the model.</p>\n<pre><code class=\"language-python\">from sklearn.metrics import mean_squared_error\r\n\r\nlin_rmse = mean_squared_error(housing_labels, housing_predictions,\r\n                              squared=False)\r\nlin_rmse\r\n>>> 68647.95686706669\n</code></pre>\n<h3>Decision Tree Regressor</h3>\n<p>So the next thing we can try is to train the model using more advanced model like Decision Tree Regressor. Let's see how it performs.</p>\n<pre><code class=\"language-python\">from sklearn.tree import DecisionTreeRegressor\r\n\r\ntree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\r\ntree_reg.fit(housing, housing_labels)\r\n\r\nhousing_predictions = tree_reg.predict(housing)\r\ntree_rmse = mean_squared_error(housing_labels, housing_predictions,\r\n                              squared=False)\r\ntree_rmse\r\n\r\n>>> 0.0\n</code></pre>\n<p>We can see that the Decision Tree Regressor has a RMSE of 0.0 which is very good on paper. But this is not a good sign because it means that the model has overfitted the data very strongly.</p>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-1\"><a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/</a><a href=\"#fnref-1\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-2\"><a href=\"https://github.com/ageron/handson-ml3\">https://github.com/ageron/handson-ml3</a><a href=\"#fnref-2\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-3\">\"Sparse Spatial Autoregressions” by Pace, R. Kelley and Ronald Barry, 1997<a href=\"#fnref-3\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>","frontmatter":{"title":"Housing prices prediction"}}},"pageContext":{"slug":"/housing-california/"}},"staticQueryHashes":["3649515864","63159454"],"slicesMap":{}}